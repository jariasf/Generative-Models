{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Author:</b> Jhosimar George Arias Figueroa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_samples = mnist.train.num_examples\n",
    "num_features = mnist.train.images.shape[1]\n",
    "print(\"Number of Samples: {}. Feature Dimension: {}\".format(num_samples, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n",
    "\n",
    "def bias_variable(shape):\n",
    "    return tf.Variable(tf.constant(0.1, shape=shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    def __init__(self, **options):\n",
    "        self.sample_size = options.get(\"sample_size\", 20)        # noise sample for generator    \n",
    "        self.batch_size = options.get(\"batch_size\", 200)         # training batch size\n",
    "        self.epochs = options.get(\"epochs\", 100)                 # number of training epochs\n",
    "        self.learning_rate = options.get(\"learning_rate\", 0.01)  # learning rate\n",
    "        self.sess = tf.Session()                                 # tensorflow session\n",
    "        self.display_step = options.get(\"display_step\", 1)       # display loss\n",
    "        self.discriminator_steps = options.get(\"k\", 1)           # k iterations for discriminator\n",
    "    \n",
    "    # Generator\n",
    "    # input: sample noise (num_samples x sample_size)\n",
    "    # output: generated image\n",
    "    def generator(self, z):\n",
    "        with tf.variable_scope(\"generator\"):\n",
    "            #first layer\n",
    "            G_W1 = weight_variable([self.sample_size, 250])\n",
    "            G_b1 = bias_variable([250])\n",
    "            G_h1 = tf.nn.tanh( tf.matmul(z, G_W1) + G_b1 )\n",
    "        \n",
    "            #second layer\n",
    "            G_W2 = weight_variable([250, 500])\n",
    "            G_b2 = bias_variable([500])\n",
    "            G_h2 = tf.nn.tanh( tf.matmul(G_h1, G_W2) + G_b2)\n",
    "        \n",
    "            #output layer\n",
    "            G_W3 = weight_variable([500, self.num_features])\n",
    "            G_b3 = bias_variable([self.num_features])\n",
    "            G_output = tf.nn.sigmoid(tf.matmul(G_h2, G_W3) + G_b3) \n",
    "        \n",
    "            return G_output\n",
    "    \n",
    "    # Discriminator\n",
    "    # input: data (num_samples x num_features)\n",
    "    # output: probabilities, logits\n",
    "    def discriminator(self, X, reuse = False):\n",
    "        with tf.variable_scope(\"discriminator\", reuse= reuse):\n",
    "            #first layer\n",
    "            D_W1 = weight_variable([self.num_features, 500])\n",
    "            D_b1 = bias_variable([500])\n",
    "            D_h1 = tf.nn.tanh( tf.matmul(X, D_W1) + D_b1)\n",
    "    \n",
    "            #second layer\n",
    "            D_W2 = weight_variable([500, 250])\n",
    "            D_b2 = bias_variable([250])\n",
    "            D_h2 = tf.nn.tanh( tf.matmul(D_h1, D_W2) + D_b2)\n",
    "    \n",
    "            #third layer\n",
    "            D_W3 = weight_variable([250, 1])\n",
    "            D_b3 = bias_variable([1])\n",
    "            D_logits = tf.matmul(D_h2, D_W3) + D_b3\n",
    "            D_output = tf.nn.sigmoid(D_logits)\n",
    "    \n",
    "            return D_output, D_logits\n",
    "    \n",
    "    @staticmethod    \n",
    "    def sample_noise(shape):\n",
    "        '''Uniform prior for G(Z)'''\n",
    "        return np.random.uniform(-1., 1., size=shape)\n",
    "    \n",
    "    # Training discriminator k times and generator once \n",
    "    def alternating_optimization(self, _X, _z):\n",
    "        #discriminator steps\n",
    "        D_loss_avg = 0\n",
    "        for i in range(self.discriminator_steps):\n",
    "            _, D_loss = self.sess.run( [self.discriminator_optimizer, self.discriminator_loss] , \n",
    "                            feed_dict = {self.X:_X, self.z:_z} )\n",
    "            D_loss_avg += D_loss\n",
    "        D_loss_avg /= self.discriminator_steps\n",
    "    \n",
    "        #generator step\n",
    "        _, G_loss = self.sess.run( [self.generator_optimizer, self.generator_loss], \n",
    "                        feed_dict= {self.z:_z})\n",
    "        \n",
    "        return D_loss_avg, G_loss\n",
    "\n",
    "    def train(self, data):\n",
    "        self.num_features = data.shape[1]\n",
    "        self.num_samples = data.shape[0]\n",
    "        \n",
    "        # Input variables\n",
    "        self.X = tf.placeholder(tf.float32, [None, self.num_features])\n",
    "        self.z = tf.placeholder(tf.float32, [None, self.sample_size])\n",
    "        \n",
    "        # Loss functions\n",
    "        self.G_sample = self.generator(self.z)\n",
    "        \n",
    "        # Discriminator loss\n",
    "        D_real, D_real_logits = self.discriminator(self.X)\n",
    "        D_fake, D_fake_logits = self.discriminator(self.G_sample, reuse = True)  \n",
    "        self.discriminator_loss = -tf.reduce_mean(tf.log(D_real) + tf.log(1.0 - D_fake))\n",
    "        #self.discriminator_loss = tf.reduce_mean(\n",
    "        #    tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real_logits, labels=tf.ones_like(D_real_logits))\n",
    "        #    + tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits,labels=tf.zeros_like(D_fake_logits)))\n",
    "\n",
    "        # Generator loss\n",
    "        self.generator_loss = -tf.reduce_mean(tf.log(D_fake))\n",
    "        #self.generator_loss = tf.reduce_mean(tf.log(1.0 - D_fake))\n",
    "        #self.generator_loss = tf.reduce_mean(\n",
    "        #    tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.one_like(D_fake_logits)))\n",
    "                                                 \n",
    "        # Shared variables\n",
    "        discriminator_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'discriminator')\n",
    "        generator_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'generator')\n",
    "    \n",
    "        # Define optimizers for both generator and discriminator\n",
    "        self.discriminator_optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        self.generator_optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        self.discriminator_optimizer = self.discriminator_optimizer.minimize(self.discriminator_loss, \n",
    "                                                                             var_list=discriminator_vars)\n",
    "        self.generator_optimizer = self.generator_optimizer.minimize(self.generator_loss,\n",
    "                                                                    var_list=generator_vars)\n",
    "       \n",
    "        # Variable initialization\n",
    "        self.sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        num_batches = int(self.num_samples/self.batch_size)\n",
    "\n",
    "        i = 0\n",
    "        # Number of iterations\n",
    "        for epoch in range(self.epochs):\n",
    "            avg_loss = 0\n",
    "            avg_generator = 0\n",
    "            avg_discriminator = 0\n",
    "            \n",
    "            # Iterate on each batch\n",
    "            for i in range(num_batches):\n",
    "                start = i * self.batch_size\n",
    "                end = i * self.batch_size + self.batch_size\n",
    "                # Get current batch\n",
    "                batch_X = data[start:end][:]\n",
    "                z = self.sample_noise([self.batch_size, self.sample_size])\n",
    "    \n",
    "                # Train GAN alternating discriminator and generator\n",
    "                D_loss, G_loss = self.alternating_optimization(batch_X, z)\n",
    "\n",
    "                avg_generator += G_loss\n",
    "                avg_discriminator += D_loss\n",
    "                avg_loss += D_loss + G_loss\n",
    "            \n",
    "            avg_discriminator /= num_batches\n",
    "            avg_generator /= num_batches\n",
    "            avg_loss /= num_batches\n",
    "            \n",
    "            if( epoch % self.display_step == 0 ):\n",
    "                print(\"Epoch {}: -- Discriminator={}, Generator={}, Loss={}\".format(epoch + 1, avg_discriminator, \n",
    "                                                                                avg_generator, avg_loss))\n",
    "\n",
    "    # Generate batch of samples\n",
    "    # input: noise (n x sample_size)\n",
    "    # output: generated images (n x num_features)\n",
    "    def generate_data(self, num_samples, noise = None):\n",
    "        if( noise == None):\n",
    "            noise = self.sample_noise([num_samples, self.sample_size])\n",
    "        generated = self.sess.run(self.G_sample, feed_dict={self.z: noise})\n",
    "        return generated\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load train data\n",
    "train_data = mnist.train.images\n",
    "\n",
    "# Generative Adversarial Network Instantiation\n",
    "GAN_model = GAN(sample_size = 100, batch_size = 100, epochs = 10000, learning_rate = 0.001, display_step = 5000 )\n",
    "\n",
    "# Training Conditional VAE\n",
    "GAN_model.train(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generated = GAN_model.generate_data(100)\n",
    "plt.figure(figsize=[10,10])\n",
    "for i in range(0,100):\n",
    "    plt.subplot(10,10,i+1)\n",
    "    plt.imshow(np.reshape(generated[i], (28, 28)), interpolation='none',cmap=plt.get_cmap('gray'))\n",
    "    plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
